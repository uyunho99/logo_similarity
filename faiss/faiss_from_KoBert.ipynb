{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H6s_QxfWnpE"
      },
      "source": [
        "모델 준비 : Kobert 모델을 준비한다.\n",
        "Load the KO-BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Uvxdtq6GWmKs"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "model = BertModel.from_pretrained(\"monologg/kobert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZDJVxw2Wx7u"
      },
      "source": [
        "Prepare your Korean language data:\n",
        "\n",
        "Collect a dataset containing Korean sentences or documents.\n",
        "Preprocess the text (e.g., remove special characters, convert to lowercase) if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BZfdmDO0P8i_"
      },
      "outputs": [],
      "source": [
        "character_limit = 300\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    sentences = []\n",
        "    for line in lines:\n",
        "        line = line.strip()  # Remove leading/trailing whitespace\n",
        "        while len(line) > character_limit:\n",
        "            sentences.append(line[:character_limit])  # Add the first 500 characters\n",
        "            line = line[character_limit:]  # Remove the first 500 characters\n",
        "        sentences.append(line)  # Add the remaining characters as a sentence\n",
        "\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I6E0fDD_WvBE"
      },
      "outputs": [],
      "source": [
        "# todo\n",
        "\n",
        "data = read_text_file(\"KCCq28_Korean_sentences_UTF8_v2.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LN5ZEK7ZP-i4"
      },
      "outputs": [],
      "source": [
        "data = data[:2000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUxBz91xW23q"
      },
      "source": [
        "Tokenize and encode the data:\n",
        "\n",
        "Tokenize the Korean text using the KO-BERT tokenizer.\n",
        "Convert the tokenized text into input features that the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U67CHBs7W4Xc"
      },
      "outputs": [],
      "source": [
        "# Tokenize and encode the data\n",
        "def encode_text(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Pad or truncate the input to a fixed length\n",
        "    max_length = 128\n",
        "    padding_length = max_length - len(input_ids)\n",
        "    input_ids = input_ids + ([0] * padding_length)\n",
        "    attention_mask = attention_mask + ([0] * padding_length)\n",
        "\n",
        "    return input_ids, attention_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ri_gPQfgW7aX"
      },
      "outputs": [],
      "source": [
        "# Encode the data\n",
        "encoded_data = [encode_text(text) for text in data]\n",
        "input_ids = [enc[0] for enc in encoded_data]\n",
        "attention_masks = [enc[1] for enc in encoded_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfJ6xJ_MXIcS"
      },
      "source": [
        "Vectorize the data using KO-BERT:\n",
        "\n",
        "Pass the encoded input through the KO-BERT model to obtain the hidden states, which represent the contextualized embeddings for each token.\n",
        "Extract the representation for the [CLS] token as the sentence or document embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hDDRPOFNXIE7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Convert the inputs to PyTorch tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "# Set the device to use\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "input_ids = input_ids.to(device)\n",
        "attention_masks = attention_masks.to(device)\n",
        "\n",
        "# Obtain the embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_masks)\n",
        "    embeddings = outputs[0][:, 0, :].cpu().numpy()  # Extract [CLS] token embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2TWISZaXOGb",
        "outputId": "bc9aff12-f458-4a4d-d19b-f7a0c2e57034"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.01500012,  0.06387398,  0.3232647 , ..., -0.21675886,\n",
              "        -0.49001923, -0.37492698],\n",
              "       [-0.13425061,  0.00895801,  0.46948475, ..., -0.04127207,\n",
              "        -0.44758508, -0.2053517 ],\n",
              "       [-0.24918352, -0.26243773,  0.36645257, ..., -0.33158505,\n",
              "        -0.25104234, -0.13303171],\n",
              "       ...,\n",
              "       [-0.112602  ,  0.16751897,  0.42021683, ..., -0.23842525,\n",
              "        -0.20740634, -0.24527523],\n",
              "       [-0.08303856,  0.1579861 ,  0.39927542, ..., -0.2115599 ,\n",
              "        -0.14765759, -0.05824763],\n",
              "       [-0.04343085,  0.05151055,  0.8291116 , ..., -0.36878407,\n",
              "         0.05955462,  0.01502977]], dtype=float32)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMD6PgrGXckn"
      },
      "source": [
        "Build the FAISS index:\n",
        "\n",
        "Create an index object with the desired configuration (e.g., index type, dimensionality).\n",
        "Add the normalized embeddings to the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FBxYcHKBXaSY"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# Build the index\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])  # Index type for inner product (cosine similarity)\n",
        "index.add(embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGjzp2RpYIAb"
      },
      "source": [
        "Perform similarity search using FAISS:\n",
        "\n",
        "Encode the query text using KO-BERT and obtain the query embedding.\n",
        "Normalize the query embedding.\n",
        "Search for similar embeddings in the FAISS index.\n",
        "Retrieve the indices and distances of the most similar embeddings.\n",
        "Retrieve the corresponding sentences or documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2y7YIsdbYLWX"
      },
      "outputs": [],
      "source": [
        "# Example: Perform similarity search\n",
        "query_text = \"그러면서 우리가 추구하는 가치와 경영이념, 임직원들의 뜻과 마음을 담아 사회공헌의 주제를 정하고, \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nW4f8NsSYGG5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "query_input = encode_text(query_text)\n",
        "query_input = [torch.tensor(tensor).to(device) for tensor in query_input]\n",
        "\n",
        "query_embedding = model(query_input[0].unsqueeze(0), attention_mask=query_input[1].unsqueeze(0))[0][:, 0, :]\n",
        "query_embedding = query_embedding.cpu().detach().numpy()\n",
        "\n",
        "normalized_query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
        "\n",
        "# Search for similar embeddings\n",
        "k = 5  # Number of similar embeddings to retrieve\n",
        "distances, indices = index.search(normalized_query_embedding, k)\n",
        "\n",
        "# Retrieve similar sentences\n",
        "similar_sentences = [data[i] for i in indices[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGDvhuQ1YWBT",
        "outputId": "6f0a3720-a141-45ec-ac7b-951a202e3bc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['그럼 순수한 어린이 관객들은 쉽게 외면하죠\"그는 왜곡된 뮤지컬이 실패하면 다시 투자를 받기 어려워지고, 이를 만회하기 위해 저가의 작품이 만들어진다며 안타까워했다.',\n",
              " '로이코 관계자에 따르면 \"805 마세라티 에디션은 단순히 마세라티의 이름을 빌린 것이 아니라 마세라티가 직접 우드를 선별할 정도로 많은 정성이 들어간 제품이다.',\n",
              " '그러면서 \"이렇게 국가적 아젠다에 대해 고민하는 것은 기본적 애국심의 발로인데 어찌해서 국민들에게 하나마나 한 맹탕 개혁이다, 졸속이다, 비열한 거래다, 이런 말로 매도 당하는지 정말 기가 막힌 심정\"이라고 강조했다.',\n",
              " '하성란씨의 \\'카레 온 더 보더\\'는 세상의 온갖 비린내를 가려주는 카레라는 음식에 기대 팍팍한 삶을 견뎌온 등장인물들을 통해 청년 실업과 노인 문제를 다룬 작품으로 \"사회성과 언어성·문학성을 정교하게 결합한 수작\"이라는 평을 들었다.',\n",
              " '그러면서 \"청렴연수원을 국제적인 반부패 교육의 허브로 육성하고, \\'공직자 부정청탁금지 및 이해충돌방지법\\' 제정에도 노력해 청탁 관행 근절을 위한 인프라를 공고히 해야 할 것\"이라고 덧붙였다.']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similar_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
